{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# trying out some benchmarking techniques\n",
    "Copyright Jim Smith,\n",
    "\n",
    "a lot of initial inspiration for code structure taken from https://machinelearningmastery.com/how-to-develop-deep-learning-models-for-univariate-time-series-forecasting/\n",
    "\n",
    "but also borrowing some ideas from here: https://towardsdatascience.com/get-started-with-using-cnn-lstm-for-forecasting-6f0f4dde5826\n",
    "\n",
    "Update 10-09-20:\n",
    "Major rewrite  to class-based version for models to tidy things up and so that I can run tests in batch mode\n",
    "\n",
    "Update 13-10-20: Soraj pointed out that there is a flaw in how the code handles test data since the option to have n_out>1 and lookAhead>1 was standardised. \n",
    "Fixed by moving all data treatment out of the models, si doing load_data followied by series ot supertvised then trian test split one oafter the other in run_Models() or the equivalent code that sets up an eperiments and runs it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "# load and plot dataset\n",
    "from math import sqrt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from pandas import DataFrame, Series\n",
    "from pandas import concat\n",
    "from pandas import read_csv\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [9.5, 6]\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import ConvLSTM2D\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_data():\n",
    "    global scaler\n",
    "    # load\n",
    "    series = read_csv('pm2.5.csv', header=0)\n",
    "    # summarize shape\n",
    "    print(series.shape)\n",
    "    n_test = int(series.shape[0]*0.1)\n",
    "    print ( 'setting n_test = {}'.format(n_test))\n",
    "    data = series.values\n",
    "    if(useLogTransform==True):\n",
    "        data = np.log(data)\n",
    "    if (useStandardisation==True):\n",
    "        data = scaler.fit_transform(data)\n",
    "    \n",
    "    return data, n_test\n",
    "\n",
    "\n",
    "# split a univariate dataset into train/test sets\n",
    "def train_test_split(data, n_test):\n",
    "    return data[:-n_test], data[-n_test:]\n",
    " \n",
    "# transform list into supervised learning format\n",
    "def series_to_supervised(data, n_in=1, n_out=1,  lookAhead=1):\n",
    "    df = DataFrame(data)\n",
    "    cols = list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(( lookAhead -1), (lookAhead-1) + n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "    # put it all together\n",
    "    agg = concat(cols, axis=1)\n",
    "    # drop rows with NaN values\n",
    "    agg.dropna(inplace=True)\n",
    "    return agg.values\n",
    " \n",
    "# root mean squared error or rmse\n",
    "def measure_rmse(actual, predicted):\n",
    "    global scaler\n",
    "    if (useStandardisation==True):\n",
    "        myactual = scaler.inverse_transform(actual)\n",
    "        mypredicted = scaler.inverse_transform(predicted)\n",
    "    else:\n",
    "        myactual = actual\n",
    "        mypredicted = predicted\n",
    "    if(useLogTransform==True):\n",
    "        myactual = np.exp(myactual)\n",
    "        mypredicted = np.exp(mypredicted)\n",
    "    return np.sqrt(mean_squared_error(myactual, mypredicted))\n",
    "\n",
    "def measure_mape(actual,predicted):\n",
    "    #if(1==1):\n",
    "    #    return 0\n",
    "    \n",
    "    minVal = 0.0001\n",
    "    global scaler\n",
    "    if (useStandardisation==True):\n",
    "        myactual = scaler.inverse_transform(actual)\n",
    "        mypredicted = scaler.inverse_transform(predicted)\n",
    "    else:\n",
    "        myactual = actual\n",
    "        mypredicted = predicted\n",
    "    if(useLogTransform==True):\n",
    "        myactual = np.exp(myactual)\n",
    "        mypredicted = np.exp(mypredicted)\n",
    "    \n",
    "    return np.mean(np.abs((myactual - mypredicted) / (myactual+minVal))) * 100\n",
    " \n",
    "# difference dataset\n",
    "def difference(data, interval):\n",
    "    return [data[i] - data[i - interval] for i in range(interval, len(data))]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# walk-forward validation for univariate data\n",
    "def walk_forward_validation(train,test, model):\n",
    "    global run\n",
    "    predictions = list()\n",
    "    # fit model\n",
    "    model.fit(train,n_epochs=maxEpochs,batchsize=batchsize)\n",
    "    \n",
    "    # seed history with training dataset\n",
    "    history = [x for x in train]\n",
    "    # step over each time-step in the test set\n",
    "    for i in range(len(test)):\n",
    "        # fit model and make forecast for history\n",
    "        yhat = model.predict(history)\n",
    "        # store forecast in list of predictions\n",
    "        predictions.append(yhat)\n",
    "        # add actual observation to history for the next loop\n",
    "        history.append(test[i])\n",
    "    # estimate prediction error\n",
    "    error = measure_rmse(test, predictions)\n",
    "    mape = measure_mape(test,predictions)\n",
    "    print(' run {}, RMSE {},  MAPE {}'.format(run,error,mape))\n",
    "\n",
    "    #save predictions to file\n",
    "    outname = 'predictions/' +str(model.lookAhead)+'hr-'+ model.name  +datapartname + '-run' + str(run) +'.csv'\n",
    "    np.savetxt(outname,predictions,delimiter=',')\n",
    "    run = run+1\n",
    "    return error, mape\n",
    "\n",
    "# summarize model performance\n",
    "def summarize_scores(name, scores):\n",
    "    # print a summary\n",
    "    scores_m, score_std = np.mean(scores), np.std(scores)\n",
    "    print('%s: %.3f RMSE (+/- %.3f)' % (name, scores_m, score_std))\n",
    "    # box and whisker plot\n",
    "    plt.boxplot(scores)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    # repeat evaluation of a config\n",
    "def repeat_evaluate(data, model, n_test, n_repeats=1):\n",
    "    # fit and evaluate the model n times\n",
    "    global run\n",
    "    run = 0\n",
    "    scores = [walk_forward_validation(data, n_test, model) for _ in range(n_repeats)]\n",
    "    return scores\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Persistence\n",
    "We will use the config to define a list of index offsets in the prior observations relative to the time to be forecasted that will be used as the prediction. For example, 12 will use the observation 12 months ago (-12) relative to the time to be forecasted.\n",
    "\n",
    "Jim's version:  predicting the value for time t calculate the median of a set of values. \n",
    "Two berhaviours are possible:\n",
    "- if intermediates==False the set is jsut the extrema $\\{ t - lookAhead, t-windowlength\\}$\n",
    "- if intermediate it is the set $\\{t-lookAhead, t-lookAhead -step, t-lookahead-2*step, \\ldots,t-windowlength\\}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# persistence\n",
    "\n",
    "class persistent_Model():\n",
    "    # fit a model\n",
    "    \n",
    "    def __init__(self,lookAhead=1,windowlength=1, intermediates=False, step=1,n_out=1):\n",
    "        self.name = 'persistent'\n",
    "        self.lookAhead = lookAhead\n",
    "        self.windowlength = windowlength\n",
    "        self.intermediates = intermediates\n",
    "        self.step=step\n",
    "        self.n_out=n_out\n",
    "\n",
    "        \n",
    "    def fit(self,train,n_epochs=1,batchsize=1):\n",
    "        return None\n",
    " \n",
    "    # forecast with a pre-fit model\n",
    "    def predict(self,history):\n",
    "        values = list()\n",
    "        if (self.intermediates == True):\n",
    "            for offset in (1, self.windowLength +1, self.step):\n",
    "                values.append(history[-offset])\n",
    "        else:\n",
    "            for offset in (1,self.windowlength):\n",
    "                values.append(history[-offset])\n",
    "        return np.median(values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP\n",
    "\n",
    "The function expects the config to be a list with the following configuration hyperparameters:\n",
    "\n",
    "- name: the algorithm name\n",
    "- n_input: The number of lag observations to use as input to the model.\n",
    "- n_nodes: The number of nodes to use in the hidden layer.\n",
    "- n_epochs: The number of times to expose the model to the whole training dataset.\n",
    "- n_batch: The number of samples within an epoch after which the weights are updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class MLP_Model():\n",
    "    def __init__(self,windowlength=1,n_nodes = 25, n_out=1, lossfcn='mse'):\n",
    "        self.name = 'MLP'\n",
    "        self.n_input = windowlength\n",
    "        self.n_out=n_out\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(n_nodes, activation='relu', input_dim=self.n_input))\n",
    "        self.model.add(Dense(self.n_out))\n",
    "        self.model.compile(loss=lossfcn, optimizer='adam')\n",
    "        \n",
    "        \n",
    "    def fit(self,train,n_epochs = 1,batchsize = 1):\n",
    "        self.batchsize = batchsize\n",
    "        train_x, train_y = train[:, :-self.n_out], train[:, -self.n_out:]\n",
    "        history = self.model.fit(train_x, train_y, epochs=n_epochs, batch_size=self.batchsize, verbose=0)\n",
    "        print('     training loss = {} '.format(history.history['loss'][n_epochs-1]))\n",
    " \n",
    "    # forecast with a pre-fit model\n",
    "    def predict(self,history):\n",
    "        # prepare data\n",
    "        x_input = np.array(history[-self.n_input:]).reshape(1, self.n_input)\n",
    "        # forecast\n",
    "        yhat = self.model.predict(x_input,batch_size=self.batchsize, verbose=0)\n",
    "        #TODO: change output to use n_out \n",
    "        return yhat[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN\n",
    "\n",
    "The model takes the following five configuration parameters as a list:\n",
    "\n",
    "- name\n",
    "- n_input: The number of lag observations to use as input to the model.\n",
    "- n_filters: The number of parallel filters.\n",
    "- n_kernel: The number of time steps considered in each read of the input sequence.\n",
    "- n_epochs: The number of times to expose the model to the whole training dataset.\n",
    "- n_batch: The number of samples within an epoch after which the weights are updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Model():\n",
    "    \n",
    "    def __init__(self,windowlength=1,n_filters = 10,kernelsize=3, n_out=1,lossfcn='mse'):\n",
    "        self.name = 'CNN'\n",
    "        self.n_input = windowlength\n",
    "        self.n_out=n_out\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Conv1D(filters=n_filters, kernel_size=kernelsize, activation='relu', input_shape=(self.n_input, 1)))\n",
    "        self.model.add(Conv1D(filters=n_filters, kernel_size=kernelsize, activation='relu'))\n",
    "        self.model.add(MaxPooling1D(pool_size=2))\n",
    "        self.model.add(Flatten())\n",
    "        self.model.add(Dense(self.n_out))\n",
    "        self.model.compile(loss=lossfcn, optimizer='adam')\n",
    "        \n",
    "    def fit(self,train,n_epochs = 1,batchsize = 1):\n",
    "        self.batchsize = batchsize\n",
    "        train_x, train_y = train[:, :-self.n_out], train[:, -self.n_out:]\n",
    "        ## next line is extra to MLP. and persistent and final 1 is number of predictions to make\n",
    "        train_x = train_x.reshape((train_x.shape[0], train_x.shape[1], 1))\n",
    "        history = self.model.fit(train_x, train_y, epochs=n_epochs, batch_size=batchsize, verbose=0)\n",
    "        print('     training loss = {} '.format(history.history['loss'][n_epochs-1]))\n",
    "        \n",
    "\n",
    "    # forecast with a pre-fit model\n",
    "    def predict(self,history):\n",
    "        # prepare data\n",
    "        x_input = np.array(history[-self.n_input:]).reshape(1, self.n_input,1)\n",
    "        # forecast\n",
    "        yhat = self.model.predict(x_input,batch_size=self.batchsize, verbose=0)\n",
    "        #TODO: change output to use n_out \n",
    "        return yhat[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM\n",
    "\n",
    "The model expects a list of  model hyperparameters; they are:\n",
    "\n",
    "- name\n",
    "- n_input: The number of lag observations to use as input to the model.\n",
    "- n_nodes: The number of LSTM units to use in the hidden layer.\n",
    "- n_epochs: The number of times to expose the model to the whole training dataset.\n",
    "- n_batch: The number of samples within an epoch after which the weights are updated.\n",
    "- n_diff: The difference order or 0 if not used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Model():\n",
    "    def __init__(self,windowlength=1,n_nodes = 25,n_diff = 0, n_out=1,lossfcn='mse'):\n",
    "        self.name = 'LSTM'\n",
    "        self.n_input = windowlength\n",
    "        self.n_nodes = n_nodes\n",
    "        self.n_diff = n_diff\n",
    "        self.n_out=n_out\n",
    "        self.model = Sequential()\n",
    "        self.model = Sequential()\n",
    "        self.model.add(LSTM(n_nodes,  input_shape=(self.n_input, 1)))\n",
    "        self.model.add(Dense(n_nodes, activation='relu'))\n",
    "        self.model.add(Dense(self.n_out)) \n",
    "        self.model.compile(loss=lossfcn, optimizer='adam')\n",
    "        \n",
    "    def fit(self,train,n_epochs = 1,batchsize = 1):\n",
    "        self.batchsize = batchsize\n",
    "        # prepare data\n",
    "        if self.n_diff > 0:\n",
    "            train = difference(train, self.n_diff)\n",
    "        train_x, train_y = train[:, :-self.n_out], train[:, -self.n_out:]\n",
    "        ## next line is extra to MLP. and persistent and final 1 is number of predictions to make\n",
    "        train_x = train_x.reshape((train_x.shape[0], train_x.shape[1], 1))\n",
    "        history = self.model.fit(train_x, train_y, epochs=n_epochs, batch_size=self.batchsize, verbose=0)\n",
    "        print('     training loss = {} '.format(history.history['loss'][n_epochs-1]))\n",
    "        \n",
    "\n",
    "    # forecast with a pre-fit model\n",
    "    def predict(self,history):\n",
    "        # prepare data\n",
    "        correction = 0.0\n",
    "        if self.n_diff > 0:\n",
    "            correction = history[-self.n_diff]\n",
    "            history = difference(history, self.n_diff)\n",
    "        x_input = np.array(history[-self.n_input:]).reshape((1, self.n_input, 1))\n",
    "        # forecast\n",
    "        yhat = self.model.predict(x_input,batch_size=self.batchsize, verbose=0)\n",
    "        #TODO: change output to use n_out \n",
    "        return correction + yhat[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacked LSTM\n",
    "\n",
    "The model expects a list of five model hyperparameters; they are:\n",
    "\n",
    "- name\n",
    "- n_input: The number of lag observations to use as input to the model.\n",
    "- n_nodes: The number of LSTM units to use in the hidden layer.\n",
    "- n_epochs: The number of times to expose the model to the whole training dataset.\n",
    "- n_batch: The number of samples within an epoch after which the weights are updated.\n",
    "- n_diff: The difference order or 0 if not used.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stacked_LSTM_Model():\n",
    "    def __init__(self,windowlength=1,n_nodes = 25,lstmlayers = 2,n_diff = 0, n_out=1,lossfcn='mse'):\n",
    "        self.name = 'stackedLSTM'\n",
    "        self.n_input = windowlength\n",
    "        self.n_nodes = n_nodes\n",
    "        self.n_diff = n_diff\n",
    "        self.lstmlayers=2\n",
    "        self.n_out=n_out\n",
    "        self.model = Sequential()\n",
    "        self.model.add(LSTM(self.n_nodes,  return_sequences=True, input_shape=(self.n_input, 1)))\n",
    "        layers = 1\n",
    "        while (layers < self.lstmlayers-1):\n",
    "            self.model.add(LSTM(self.n_nodes,  return_sequences=True))\n",
    "            layers = layers+1\n",
    "        self.model.add(LSTM(self.n_nodes))    \n",
    "        self.model.add(Dense(self.n_nodes, activation='relu'))\n",
    "        self.model.add(Dense(self.n_out))\n",
    "        self.model.compile(loss=lossfcn, optimizer='adam')\n",
    "\n",
    "        \n",
    "    def fit(self,train,n_epochs = 1,batchsize = 1):\n",
    "        self.batchsize = batchsize\n",
    "        # prepare data\n",
    "        if self.n_diff > 0:\n",
    "            train = difference(train, self.n_diff)\n",
    "        train_x, train_y = train[:, :-self.n_out], train[:, -self.n_out:]\n",
    "        ## next line is extra to MLP. and persistent and final 1 is number of predictions to make\n",
    "        train_x = train_x.reshape((train_x.shape[0], train_x.shape[1], 1))\n",
    "        history = self.model.fit(train_x, train_y, epochs=n_epochs, batch_size=batchsize, verbose=0)\n",
    "        print('     training loss = {} '.format(history.history['loss'][n_epochs-1]))\n",
    "        \n",
    "\n",
    "    # forecast with a pre-fit model\n",
    "    def predict(self,history):\n",
    "        # prepare data\n",
    "        correction = 0.0\n",
    "        if self.n_diff > 0:\n",
    "            correction = history[-self.n_diff]\n",
    "            history = difference(history, self.n_diff)\n",
    "        x_input = np.array(history[-self.n_input:]).reshape((1, self.n_input, 1))\n",
    "        # forecast\n",
    "        yhat = self.model.predict(x_input,batch_size=self.batchsize, verbose=0)\n",
    "        #TODO: change output to use n_out \n",
    "        return correction + yhat[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN LSTM\n",
    "The model expects a list of seven hyperparameters; they are:\n",
    "\n",
    "- name,\n",
    "- n_seq: The number of subsequences within a sample.\n",
    "- n_steps: The number of time steps within each subsequence.\n",
    "- n_filters: The number of parallel filters.\n",
    "- n_kernel: The number of time steps considered in each read of the input sequence.\n",
    "- n_nodes: The number of LSTM units to use in the hidden layer.\n",
    "- n_epochs: The number of times to expose the model to the whole training dataset.\n",
    "- n_batch: The number of samples within an epoch after which the weights are updated.\n",
    "\n",
    "config's affected by this blog post: https://towardsdatascience.com/get-started-with-using-cnn-lstm-for-forecasting-6f0f4dde5826\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_LSTM_Model():\n",
    "    def __init__(self,n_seq=1,windowlength=1,n_filters = 25,kernelsize=3,lstmnodes=25,dense_nodes=25, n_out=1,lossfcn='mse'):\n",
    "        self.name = 'CNN_LSTM'\n",
    "        self.n_seq=n_seq\n",
    "        self.n_steps = windowlength\n",
    "        self.n_input = n_seq*windowlength\n",
    "        self.n_filters = n_filters\n",
    "        self.kernelsize = kernelsize\n",
    "        self.n_lstmnodes = lstmnodes\n",
    "        self.n_densenodes=dense_nodes\n",
    "        self.n_out=n_out\n",
    "        self.model = Sequential()\n",
    "        self.model.add(TimeDistributed(Conv1D(filters=self.n_filters, kernel_size=self.kernelsize, activation='relu', input_shape=(self.n_steps,None,self.n_steps,1))))\n",
    "        self.model.add(TimeDistributed(Conv1D(filters=self.n_filters, kernel_size=self.kernelsize, activation='relu')))\n",
    "        self.model.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n",
    "        self.model.add(TimeDistributed(Flatten()))\n",
    "        self.model.add(LSTM(self.n_lstmnodes, activation='relu'))\n",
    "        self.model.add(Dense(self.n_densenodes, activation='relu'))\n",
    "        self.model.add(Dense(self.n_out))\n",
    "        self.model.compile(loss=lossfcn, optimizer='adam')\n",
    "\n",
    "        \n",
    "    def fit(self,train,n_epochs = 1,batchsize = 1):\n",
    "        self.batchsize = batchsize\n",
    "        # prepare data\n",
    "        train_x, train_y = train[:, :-self.n_out], train[:, -self.n_out:]\n",
    "        ## next line is extra to MLP adn nort same as CNN or lstm\n",
    "        train_x = train_x.reshape((train_x.shape[0], self.n_seq, self.n_steps, 1))\n",
    "        history = self.model.fit(train_x, train_y, epochs=n_epochs, batch_size=batchsize, verbose=0)\n",
    "        print('     training loss = {} '.format(history.history['loss'][n_epochs-1]))\n",
    "        \n",
    "\n",
    "    # forecast with a pre-fit model\n",
    "    def predict(self,history):\n",
    "        # prepare data\n",
    "        x_input = np.array(history[-self.n_input:]).reshape((1, self.n_seq, self.n_steps, 1))\n",
    "        # forecast\n",
    "        yhat = self.model.predict(x_input,batch_size=self.batchsize, verbose=0)\n",
    "        #TODO: change output to use n_out \n",
    "        return  yhat[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conv-LSTM\n",
    "\n",
    "|The model expects a list of  hyperparameters, the same as the CNN-LSTM; they are:\n",
    "\n",
    "- name\n",
    "- n_out: the number of time steps ahead predicted simultaneously\n",
    "- n_seq: The number of subsequences within a sample.\n",
    "- n_steps: The number of time steps within each subsequence.\n",
    "- n_filters: The number of parallel filters.\n",
    "- n_kernel: The number of time steps considered in each read of the input sequence.\n",
    "- n_nodes: The number of LSTM units to use in the hidden layer.\n",
    "- n_epochs: The number of times to expose the model to the whole training dataset.\n",
    "- n_batch: The number of samples within an epoch after which the weights are updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLSTM_Model():\n",
    "\n",
    "    def __init__(self,n_out=1,n_seq=1,windowlength=1,n_filters = 25,kernelsize=3,dense_nodes=25,lossfcn='mse'):\n",
    "        self.name = 'ConvLSTM'\n",
    "        self.dimensions=1\n",
    "        self.n_seq=n_seq\n",
    "        self.n_steps = windowlength\n",
    "        self.n_input = n_seq*windowlength\n",
    "        self.n_filters = n_filters\n",
    "        self.kernelsize = kernelsize\n",
    "        self.kernelshape = (self.dimensions,self.kernelsize)\n",
    "        self.n_densenodes=dense_nodes\n",
    "        self.n_out=n_out\n",
    "        \n",
    "#TODO:check whether dimension size should be n_out??\n",
    "           # define model\n",
    "        self.model = Sequential()\n",
    "        self.model.add(ConvLSTM2D(filters=self.n_filters, kernel_size=self.kernelshape, activation='relu', input_shape=(self.n_seq, self.dimensions, self.n_steps, 1)))\n",
    "        self.model.add(Flatten())\n",
    "        self.model.add(Dense(self.n_densenodes, activation='relu'))\n",
    "        self.model.add(Dense(self.n_out))\n",
    "        self.model.compile(loss=lossfcn, optimizer='adam')\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "    def fit(self,train,n_epochs = 1,batchsize = 1):\n",
    "        self.batchsize = batchsize\n",
    "        # prepare data\n",
    "        train_x, train_y = train[:, :-self.n_out], train[:, -self.n_out:]\n",
    "        ## next line is extra to MLP and is not the same as CNN or lstm\n",
    "        train_x = train_x.reshape((train_x.shape[0], self.n_seq,self.dimensions, self.n_steps, 1))\n",
    "        history = self.model.fit(train_x, train_y, epochs=n_epochs, batch_size=batchsize, verbose=0)\n",
    "        print('     training loss = {} '.format(history.history['loss'][n_epochs-1]))\n",
    "        \n",
    "\n",
    "    # forecast with a pre-fit model\n",
    "    def predict(self,history):\n",
    "        # prepare data\n",
    "        x_input = np.array(history[-self.n_input:]).reshape((1, self.n_seq, self.dimensions,self.n_steps, 1))\n",
    "        # forecast\n",
    "        yhat = self.model.predict(x_input,batch_size=self.batchsize, verbose=0)\n",
    "        #TODO: change output to use n_out \n",
    "        return  yhat[0]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing Switches and experimental settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO make these passed through main() as options OR put them in a loop\n",
    "\n",
    "#preprocessing\n",
    "useStandardisation = False\n",
    "useLogTransform = False\n",
    "datapartname = \"\"\n",
    "scaler = StandardScaler()\n",
    "\n",
    "\n",
    "\n",
    "## model defaults\n",
    "#size of window presented\n",
    "oneDayLag = 24 \n",
    "threeDayLag = 72\n",
    "windowSize = oneDayLag\n",
    "nsubseq=1 #¢ the number of subsequences that we split our inputr into \n",
    "# number o periods ahead predicted\n",
    "lookAhead =24 # the number of periods into the future that the prediction should be\n",
    "##i.e. 1=next hour, 24 = this hour, tomorrow\n",
    "n_out = 1 # the number of predictions to make\n",
    "\n",
    "\n",
    "\n",
    "hidden_nodes =  50 #200#use this for the number of CNN filters as well\n",
    "maxEpochs = 25 # from experience withthis data\n",
    "batchsize = 168  # one week\n",
    "\n",
    "mylossfcn = 'mean_absolute_percentage_error'\n",
    "run = 0\n",
    "Repetitions = 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualise Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data,_ = load_data()\n",
    "\n",
    "# plot\n",
    "plt.plot(data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 'debug' cell to check dimension of things being passed around"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data, n_test = load_data()\n",
    "#inputsize = windowSize*nsubseq\n",
    "#    aggdata = series_to_supervised(data, n_in=inputsize, n_out=n_out,  lookAhead=lookAhead)\n",
    "#    train, test = train_test_split(aggdata, n_test)\n",
    "#maxEpochs =100\n",
    "#Repetitions=1\n",
    "#model = MLP_Model( windowlength=windowSize, n_nodes = hidden_nodes,lossfcn=mylossfcn)\n",
    "#model = LSTM_Model( windowlength=windowSize, n_nodes = hidden_nodes, n_diff = 0,lossfcn=mylossfcn)\n",
    "#scores = repeat_evaluate(data, model, n_test,n_repeats=Repetitions)\n",
    "#scorearray = np.array(scores)\n",
    "#print (scorearray[:,0], scorearray[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run each different model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Run_Models(useLogs=False, useStandard=False, epochs=2,repeats=2):\n",
    "    global datapartname, useLogTransform, useStandardisation\n",
    "    useLogTransform = useLogs\n",
    "    useStandardisation = useStandard\n",
    "    print(useLogs,useStandard)\n",
    "    datapartname = '-raw'\n",
    "    if (useLogTransform==True):\n",
    "        datapartname = '-log'    \n",
    "    if (useStandardisation == True):\n",
    "        datapartname = datapartname + '-std'\n",
    "    print(\"=======> running: \",datapartname)\n",
    "    data, n_test = load_data()\n",
    "    inputsize = windowSize*nsubseq\n",
    "    aggdata = series_to_supervised(data, n_in=inputsize, n_out=n_out,  lookAhead=lookAhead)\n",
    "    train, test = train_test_split(aggdata, n_test)\n",
    "    \n",
    "\n",
    "    mseresults = DataFrame()\n",
    "    maperesults = DataFrame()\n",
    "    colnames = []\n",
    "\n",
    "    model_list = []\n",
    "    model_list.append(  persistent_Model(lookAhead= lookAhead, windowlength=windowSize))\n",
    "    model_list.append(         MLP_Model(lookAhead= lookAhead, windowlength=windowSize, n_nodes = hidden_nodes,lossfcn=mylossfcn)   )\n",
    "    model_list.append(         CNN_Model(lookAhead= lookAhead, windowlength=windowSize, n_filters = hidden_nodes, kernelsize=3,lossfcn=mylossfcn) )\n",
    "    model_list.append(        LSTM_Model(lookAhead= lookAhead, windowlength=windowSize, n_nodes = hidden_nodes, n_diff = 0,lossfcn=mylossfcn)  )\n",
    "    model_list.append(Stacked_LSTM_Model(lookAhead= lookAhead, windowlength=windowSize, n_nodes = hidden_nodes,lstmlayers = 2,n_diff = 0,lossfcn=mylossfcn))\n",
    "    model_list.append(    CNN_LSTM_Model(lookAhead= lookAhead, windowlength=windowSize, n_seq=nsubseq,n_filters = hidden_nodes,kernelsize=3,lstmnodes=hidden_nodes,dense_nodes=hidden_nodes,lossfcn=mylossfcn))\n",
    "    model_list.append(    ConvLSTM_Model(lookAhead= lookAhead, windowlength=windowSize, n_seq=nsubseq,n_filters = hidden_nodes,kernelsize=3,dense_nodes=hidden_nodes,lossfcn=mylossfcn))\n",
    "\n",
    "    for model in model_list:\n",
    "        print (\"running \", model.name)\n",
    "        scores = repeat_evaluate(train,test, model, n_repeats=Repetitions)\n",
    "        scorearray = np.array(scores)\n",
    "        mseresults[model.name]= Series(scorearray[:,0])\n",
    "        maperesults[model.name] = Series(scorearray[:,1])\n",
    "        colnames.append(model.name)\n",
    "\n",
    "    outfilename = 'results/aggregated_Univariate_Results_mse_' + str(lookAhead) +'hr_'+ datapartname +'data.csv'\n",
    "    mseresults.to_csv(outfilename)\n",
    "    outfilename = 'results/aggregated_Univariate_Results_mape_' + str(lookAhead) +'hr_'+ datapartname +'data.csv'\n",
    "    maperesults.to_csv(outfilename)\n",
    "\n",
    "    mseplot = mseresults.boxplot(column=colnames)\n",
    "    title = 'Boxplot of mse_results from 10 runs with ' +datapartname + ' univariate data'\n",
    "    plt.title(title)\n",
    "    plt.ylabel('RMSE in original scale')\n",
    "    filename = 'results/'+title+'.jpg'\n",
    "    plt.savefig(filename)\n",
    "     \n",
    "    mapeplot = maperesults.boxplot(column=colnames)\n",
    "    title = 'Boxplot of mape_results from 10 runs with ' +datapartname + ' univariate data'\n",
    "    plt.title(title)\n",
    "    plt.ylabel('MAPE in original scale')\n",
    "    filename = 'results/'+title+'.jpg'\n",
    "    plt.savefig(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mylog in (False,True):\n",
    "    for myscaled in (False,True ):\n",
    "        Run_Models(useLogs=mylog, useStandard=myscaled, epochs=1,repeats=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SaveData(useLogs=False, useStandard=False, epochs=2,repeats=2):\n",
    "    global datapartname, useLogTransform, useStandardisation\n",
    "    useLogTransform = useLogs\n",
    "    useStandardisation = useStandard\n",
    "    print(useLogs,useStandard)\n",
    "    datapartname = '-raw'\n",
    "    if (useLogTransform==True):\n",
    "        datapartname = '-log'    \n",
    "    if (useStandardisation == True):\n",
    "        datapartname = datapartname + '-std'\n",
    "    print(\"=======> running: \",datapartname)\n",
    "    data, n_test = load_data()\n",
    "    train,test = train_test_split(data,n_test)\n",
    "    #save train and test to file\n",
    "    outname = 'train'  +datapartname +'.csv'\n",
    "    np.savetxt(outname,train,delimiter=',')\n",
    "    outname = 'test'  +datapartname +'.csv'\n",
    "    np.savetxt(outname,test,delimiter=',')\n",
    "        \n",
    "for useLogs in (True,False):\n",
    "    for useStandard in (True,False):\n",
    "        SaveData(useLogs,useStandard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data, n_test = load_data()\n",
    "#scores = repeat_evaluate(data, MLP_Model(lookAhead= lookAhead, windowlength=windowSize, n_nodes = hidden_nodes) , n_test,n_repeats=Repetitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
